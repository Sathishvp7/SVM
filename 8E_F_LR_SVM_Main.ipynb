{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HExLQrE4ZxR"
   },
   "source": [
    "<h1><font color='blue'> 8E and 8F: Finding the Probability P(Y==1|X)</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LuKrFzC4ZxV"
   },
   "source": [
    "<h2><font color='Geen'> 8E: Implementing Decision Function of SVM RBF Kernel</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1wES-wWN4ZxX"
   },
   "source": [
    "<font face=' Comic Sans MS' size=3>After we train a kernel SVM model, we will be getting support vectors and their corresponsing coefficients $\\alpha_{i}$\n",
    "\n",
    "Check the documentation for better understanding of these attributes: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "<img src='https://i.imgur.com/K11msU4.png' width=500>\n",
    "\n",
    "As a part of this assignment you will be implementing the ```decision_function()``` of kernel SVM, here decision_function() means based on the value return by ```decision_function()``` model will classify the data point either as positive or negative\n",
    "\n",
    "Ex 1: In logistic regression After traning the models with the optimal weights $w$ we get, we will find the value $\\frac{1}{1+\\exp(-(wx+b))}$, if this value comes out to be < 0.5 we will mark it as negative class, else its positive class\n",
    "\n",
    "Ex 2: In Linear SVM After traning the models with the optimal weights $w$ we get, we will find the value of $sign(wx+b)$, if this value comes out to be -ve we will mark it as negative class, else its positive class.\n",
    "\n",
    "Similarly in Kernel SVM After traning the models with the coefficients $\\alpha_{i}$ we get, we will find the value of \n",
    "$sign(\\sum_{i=1}^{n}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here $K(x_{i},x_{q})$ is the RBF kernel. If this value comes out to be -ve we will mark $x_{q}$ as negative class, else its positive class.\n",
    "\n",
    "RBF kernel is defined as: $K(x_{i},x_{q})$ = $exp(-\\gamma ||x_{i} - x_{q}||^2)$\n",
    "\n",
    "For better understanding check this link: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z830CfMk4Zxa"
   },
   "source": [
    "## Task E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MuBxHiCQ4Zxc"
   },
   "source": [
    "> 1. Split the data into $X_{train}$(60), $X_{cv}$(20), $X_{test}$(20)\n",
    "\n",
    "> 2. Train $SVC(gamma=0.001, C=100.)$ on the ($X_{train}$, $y_{train}$)\n",
    "\n",
    "> 3. Get the decision boundry values $f_{cv}$ on the $X_{cv}$ data  i.e. ` `$f_{cv}$ ```= decision_function(```$X_{cv}$```)```  <font color='red'>you need to implement this decision_function()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCgMNEvI4Zxf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, \n",
    "                           random_state=15)\n",
    "\n",
    "train_X,test_X,train_y,test_y  = train_test_split(X,y,test_size=0.20,random_state=42)\n",
    "train_X,train_CV,train_y,test_CV  = train_test_split(train_X,train_y,test_size=0.20,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.DataFrame(test_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    569\n",
       "1    231\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANUNIqCe4Zxn"
   },
   "source": [
    "### Pseudo code\n",
    "\n",
    "clf = SVC(gamma=0.001, C=100.)<br>\n",
    "clf.fit(Xtrain, ytrain)\n",
    "\n",
    "<font color='green'>def</font> <font color='blue'>decision_function</font>(Xcv, ...): #use appropriate parameters <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='green'>for</font> a data point $x_q$ <font color='green'>in</font> Xcv: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='grey'>#write code to implement $(\\sum_{i=1}^{\\text{all the support vectors}}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here the values $y_i$, $\\alpha_{i}$, and $intercept$ can be obtained from the trained model</font><br>\n",
    "   <font color='green'>return</font> <font color='grey'><i># the decision_function output for all the data points in the Xcv</i></font>\n",
    "    \n",
    "fcv = decision_function(Xcv, ...)  <i># based on your requirement you can pass any other parameters </i>\n",
    "\n",
    "<b>Note</b>: Make sure the values you get as fcv, should be equal to outputs of clf.decision_function(Xcv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = SVC(gamma=0.001, C=100.0, probability=True)\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "model_dec_value = model.decision_function(train_CV)\n",
    "\n",
    "prams= model.get_params()\n",
    "b =  model.intercept_\n",
    "sv = model.support_vectors_\n",
    "a = model.dual_coef_\n",
    "\n",
    "custom_dec_val = []\n",
    "\n",
    "def decision_function(x_cv, c, gamma):\n",
    "    #print(x_cv)\n",
    "    for cv in x_cv:\n",
    "    #for i, cv in tqdm(enumerate(x_cv)):\n",
    "        value = 0\n",
    "        for j in range(len(sv)):\n",
    "            l2_norm = np.linalg.norm(cv - sv[j])\n",
    "            kernel = np.exp(-prams['gamma'] * (l2_norm**2))\n",
    "            value += a[0][j] * kernel\n",
    "        value = value + b\n",
    "        custom_dec_val.append(value[0])\n",
    "        \n",
    "fcv = decision_function(train_CV, 100.0, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.001, 'kernel': 'rbf', 'max_iter': -1, 'probability': True, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "[-1.80151873]\n",
      "[[-0.45249961 -0.55624639 -0.18877168 -0.26346616  0.11359858]\n",
      " [ 0.71918122 -0.90416915 -0.18571414 -0.27647336 -0.18785388]\n",
      " [-0.97685553  0.1003584  -0.09967914 -0.12004848  0.39077859]\n",
      " ...\n",
      " [ 1.16313433 -0.29206536 -0.05199842 -0.07929317 -0.08525518]\n",
      " [ 0.21190377 -0.01954856 -0.01010565 -0.01360925  0.014668  ]\n",
      " [ 0.48453654  0.00724127  0.03051939  0.03859359 -0.08777591]]\n",
      "[[-100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.          -89.31854809 -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.          -65.10730384 -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.         -100.\n",
      "  -100.         -100.         -100.         -100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.           77.71148322\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.           66.95100566  100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.           75.76177188  100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.           34.00159118\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.          100.\n",
      "   100.          100.          100.          100.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(prams)\n",
    "print(b)\n",
    "print(sv)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cf_inbuilt</th>\n",
       "      <th>manual_cf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.877196</td>\n",
       "      <td>0.877196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.120046</td>\n",
       "      <td>-1.120046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.563691</td>\n",
       "      <td>-3.563691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.631567</td>\n",
       "      <td>-0.631567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.192167</td>\n",
       "      <td>-1.192167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cf_inbuilt  manual_cf\n",
       "0    0.877196   0.877196\n",
       "1   -1.120046  -1.120046\n",
       "2   -3.563691  -3.563691\n",
       "3   -0.631567  -0.631567\n",
       "4   -1.192167  -1.192167"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_decision_fun = pd.DataFrame(data= [model.decision_function(train_CV),custom_dec_val]).T\n",
    "#sample results\n",
    "compare_decision_fun.rename({0:'Cf_inbuilt',1:'manual_cf'},axis=1)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tHie1zqH4Zxt"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h43kDT3M41u5"
   },
   "source": [
    "<strong>As we have binary target variables, Decision function concludes as when the data points gives postive points which in the postive side of hyperplane , simlarly negative means Hyperplane in negative direction </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c0bKCboN4Zxu"
   },
   "source": [
    "<h2><font color='Geen'> 8F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nMn7OEN94Zxw"
   },
   "source": [
    "Check this <a href='https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a'>PDF</a>\n",
    "<img src='https://i.imgur.com/CAMnVnh.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0n5EFkx4Zxz"
   },
   "source": [
    "## TASK F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t0HOqVJq4Zx1"
   },
   "source": [
    "\n",
    "> 4. Apply SGD algorithm with ($f_{cv}$, $y_{cv}$) and find the weight $W$ intercept $b$ ```Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)``` \n",
    "\n",
    "> Note1: Don't forget to change the values of $y_{cv}$ as mentioned in the above image. you will calculate y+, y- based on data points in train data\n",
    "\n",
    "> Note2: the Sklearn's SGD algorithm doesn't support the real valued outputs, you need to use the code that was done in the `'Logistic Regression with SGD and L2'` Assignment after modifying loss function, and use same parameters that used in that assignment.\n",
    "<img src='https://i.imgur.com/zKYE9Oc.png'>\n",
    "if Y[i] is 1, it will be replaced with y+ value else it will replaced with y- value\n",
    "\n",
    "> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oTY7z2bd4Zx2"
   },
   "source": [
    "__Note: in the above algorithm, the steps 2, 4 might need hyper parameter tuning, To reduce the complexity of the assignment we are excluding the hyerparameter tuning part, but intrested students can try that__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CM3odN1Z4Zx3"
   },
   "source": [
    "\n",
    "If any one wants to try other calibration algorithm istonic regression also please check these tutorials\n",
    "\n",
    "1. http://fa.bianp.net/blog/tag/scikit-learn.html#fn:1\n",
    "\n",
    "2. https://drive.google.com/open?id=1MzmA7QaP58RDzocB0RBmRiWfl7Co_VJ7\n",
    "\n",
    "3. https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a\n",
    "\n",
    "4. https://stat.fandom.com/wiki/Isotonic_regression#Pool_Adjacent_Violators_Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Y+ and y-\n",
    "N_pos =0\n",
    "N_neg = 0\n",
    "for i in test_CV:\n",
    "    if i ==1:\n",
    "        N_pos+=1\n",
    "    else:\n",
    "        N_neg+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pos = (N_pos+1)/(N_pos+2)\n",
    "Y_neg  =1/(N_neg+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Postive test CV with Y_pos and 0 with Y_neg\n",
    "Updated_testCV = []\n",
    "for i in test_CV:\n",
    "    if i ==1:\n",
    "        Updated_testCV.append(Y_pos)\n",
    "    else:\n",
    "        Updated_testCV.append(Y_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_weights():\n",
    "    w = np.zeros(1)\n",
    "    b = 0\n",
    "    return w,b\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logloss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for index in range(len(y_true)):\n",
    "        a = (y_true[index] * math.log(y_pred[index], 10)) + \\\n",
    "        (1 - y_true[index]) * math.log(1 - y_pred[index], 10)\n",
    "        b = (-1/len(y_true))\n",
    "        loss = loss + a * b\n",
    "    return loss\n",
    "\n",
    "def gradient_dw(x, y, w, b):\n",
    "    error = y - sigmoid(np.dot(x, w.T) + b)\n",
    "    dw = x * error\n",
    "    return dw\n",
    "\n",
    "def gradient_db(x, y, w, b):\n",
    "    db = y - sigmoid(np.dot(x, w.T) + b)\n",
    "    return db\n",
    "\n",
    "def probability(x, w, b):\n",
    "    predicted = []\n",
    "    probability = []\n",
    "    for i in range(len(x)):\n",
    "        z = np.dot(w, x[i]) + b\n",
    "        sig = sigmoid(z)\n",
    "        if sig >= 0.5:\n",
    "            predicted.append(1)\n",
    "        else:\n",
    "            predicted.append(0)\n",
    "        probability.append(sig)\n",
    "    return np.array(predicted), np.array(probability)\n",
    "\n",
    "def find_accuracy(actual, predicted):\n",
    "    return round((np.sum((actual == predicted) / \n",
    "                                      len(actual))) * 100, 2)\n",
    "\n",
    "def train(X_test, y_test, epochs, alpha, lr):\n",
    "    \n",
    "    \n",
    "    global test_loss\n",
    "    test_loss = []\n",
    "    \n",
    "    w, b = initialize_weights()\n",
    "    for ep in range(epochs):\n",
    "        print('Epoch:', ep + 1)\n",
    "        for index in range(len(X_test)):\n",
    "            r_index = random.randint(0, len(X_test) - 1)\n",
    "            \n",
    "            ln_eqn = np.dot(X_test[r_index], w.T) + b\n",
    "            error = y_test[r_index] - sigmoid(ln_eqn)\n",
    "            \n",
    "            dw = X_test[r_index] * error\n",
    "            db = error\n",
    "            \n",
    "            w = w + (alpha * dw)\n",
    "            b = b + (alpha * db)\n",
    "        \n",
    "        \n",
    "        predicted, score = probability(X_test, w, b)\n",
    "        loss = logloss(y_test, score)\n",
    "        test_loss.append(loss)\n",
    "        te_acc = find_accuracy(y_test, predicted)\n",
    "        print('Accuracy',te_acc)\n",
    "\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Accuracy 71.12\n",
      "Epoch: 2\n",
      "Accuracy 71.12\n",
      "Epoch: 3\n",
      "Accuracy 71.12\n",
      "Epoch: 4\n",
      "Accuracy 71.12\n",
      "Epoch: 5\n",
      "Accuracy 71.12\n",
      "Epoch: 6\n",
      "Accuracy 71.12\n",
      "Epoch: 7\n",
      "Accuracy 71.12\n",
      "Epoch: 8\n",
      "Accuracy 71.12\n",
      "Epoch: 9\n",
      "Accuracy 71.12\n",
      "Epoch: 10\n",
      "Accuracy 71.12\n"
     ]
    }
   ],
   "source": [
    "lr  = 0.0001\n",
    "alpha = 0.0001\n",
    "N = len(Updated_testCV)\n",
    "epochs = 10 \n",
    "\n",
    "w, b = train(Updated_testCV, test_CV, epochs, alpha, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11633561])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15534377])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate probablities\n",
    "prob_values_calibrate = []\n",
    "for (test_value) in custom_dec_val:\n",
    "    #print(test_value)\n",
    "    kernel = np.exp((-(w * (test_value+b))))\n",
    "    prob = (1/(1+kernel))\n",
    "    #print(prob)\n",
    "    prob_values_calibrate.append((prob))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prob_values_calibrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prob_calibriate = pd.DataFrame(data={'Prob_score_1':prob_values_calibrate,'Actual_testCV':test_CV})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prob_calibriate['Prob_score_1'] = df_prob_calibriate['Prob_score_1'].str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prob_score_1</th>\n",
       "      <th>Actual_testCV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.520982</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.462975</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.393492</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.477130</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.460889</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prob_score_1  Actual_testCV\n",
       "0      0.520982              1\n",
       "1      0.462975              0\n",
       "2      0.393492              0\n",
       "3      0.477130              0\n",
       "4      0.460889              0"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prob_calibriate.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>The empirical results show that after calibration gives best probablities which matches actual test_cv</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "8E&F_LR_SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
